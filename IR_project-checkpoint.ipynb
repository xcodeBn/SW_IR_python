{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69164049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tabl', 'gum', 'pc', 'pen']\n",
      "Please enter the query:\n",
      "gum  pen  pc\n",
      "[0, 1, 1, 1]\n",
      "[0.0, 0.09691001300805642, 0.2218487496163564, 0.3979400086720376]\n",
      "[{'fileName': 'test1.TXT', 'data': 'gum  \\t', 'stemming_data': ['gum'], 'tf_data': [0, 1, 0, 0], 'tfidf_data': [0.0, 0.09691001300805642, 0.0, 0.0]}, {'fileName': 'test2.TXT', 'data': 'gum  pc  table', 'stemming_data': ['gum', 'pc', 'tabl'], 'tf_data': [1, 1, 1, 0], 'tfidf_data': [0.3979400086720376, 0.09691001300805642, 0.2218487496163564, 0.0]}, {'fileName': 'test3.TXT', 'data': 'gum  gum  pen  pc', 'stemming_data': ['gum', 'gum', 'pen', 'pc'], 'tf_data': [0, 2, 1, 1], 'tfidf_data': [0.0, 0.19382002601611284, 0.2218487496163564, 0.3979400086720376]}, {'fileName': 'test4.TXT', 'data': 'gum  pen  pc  ', 'stemming_data': ['gum', 'pen', 'pc'], 'tf_data': [0, 1, 1, 1], 'tfidf_data': [0.0, 0.09691001300805642, 0.2218487496163564, 0.3979400086720376]}, {'fileName': 'test5.TXT', 'data': 'table  \\t', 'stemming_data': ['tabl'], 'tf_data': [1, 0, 0, 0], 'tfidf_data': [0.3979400086720376, 0.0, 0.0, 0.0]}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import functools\n",
    "import math\n",
    "from nltk.stem import PorterStemmer\n",
    "from operator import itemgetter\n",
    "with open(\"./stop_words_english.txt\", \"r\",encoding=\"utf8\") as readerst:\n",
    "        datastop_words = readerst.read()    \n",
    "        stop_words = datastop_words.lower().replace(\"\\n\", \" \").split(\" \")\n",
    "files_list = []\n",
    "\n",
    "def filter(data):       \n",
    "    list_of_words = data.lower().replace(\"\\n\", \" \").split(\" \")\n",
    "    filter_list=[word for word in list_of_words if word not in stop_words and len(word) > 1]\n",
    "\n",
    "    for elf in filter_list:\n",
    "        if\".net\"in elf:continue\n",
    "        elif '@'in elf:\n",
    "                if \".com\"in elf or \".lb\" in elf or \".gov\" in elf or \".edu\" in elf:\n",
    "                    continue\n",
    "                else:\n",
    "                    filter_list.remove(elf)    \n",
    "        else:\n",
    "                continue\n",
    "    final_filter_list=[]\n",
    "    for dot in filter_list:\n",
    "            if  dot[-1]==\".\" or dot[-1]==\",\" or dot[-1]==\":\":\n",
    "                dotx = dot[:-1]\n",
    "                dot=dotx\n",
    "                final_filter_list.append(dot)\n",
    "            elif \"'\" in dot:\n",
    "                filter_list.remove(dot)\n",
    "            else:\n",
    "                final_filter_list.append(dot)\n",
    "    filter_list=final_filter_list\n",
    "    \n",
    "    return filter_list\n",
    "\n",
    "def stemming(filter_data):\n",
    "    porter = PorterStemmer()\n",
    "    sufix_list=[(porter.stem(word)) for word in filter_data]\n",
    "    return sufix_list\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for filename in files:\n",
    "        if \"test\" in filename:\n",
    "            file_path = os.path.join(root,filename)\n",
    "            file = open(file_path, 'r')\n",
    "            files_list.append({'fileName': filename, 'data': file.read()})\n",
    "            file.close()\n",
    "def multiply(list1,list2):\n",
    "    products = []\n",
    "    for num1,num2 in zip(list1,list2):\n",
    "        products.append((num1) * num2)\n",
    "    return products \n",
    "\n",
    "def count_key(key,dict_list):\n",
    "    keys_list = []\n",
    "    for item in dict_list:\n",
    "        keys_list += item.keys()\n",
    "    return keys_list.count(key)   \n",
    "\n",
    "\n",
    "\n",
    "for doc in files_list:\n",
    "    stemming_data = stemming(filter(doc['data']))\n",
    "    doc[\"stemming_data\"] = stemming_data \n",
    "    \n",
    "  \n",
    "    \n",
    "vocab = list(functools.reduce(lambda x ,y : set(x).union(y['stemming_data']), files_list, set()))    \n",
    "\n",
    "print(vocab)\n",
    "    \n",
    "\n",
    "    \n",
    "tf_data_list=[]   \n",
    "for doc in files_list:\n",
    "    stemming_data = doc[\"stemming_data\"]\n",
    "    tf_data = [stemming_data.count(word)for word in vocab]\n",
    "    tf_data_list.append(tf_data)\n",
    "    doc[\"tf_data\"] = tf_data \n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "DF = [sum([1 for i in tf_data_list if i[b] >0]) for b in range(len(tf_data_list[0]))]       \n",
    "IDF=[math.log10(count_key(\"fileName\",files_list)/d)for d in DF] \n",
    "\n",
    "for doc in files_list:\n",
    "    tf_data = doc[\"tf_data\"]\n",
    "    tfidf_data = multiply(tf_data,IDF)\n",
    "    doc[\"tfidf_data\"] = tfidf_data\n",
    "query = input(\"Please enter the query:\\n\")    \n",
    "stemming_query = stemming(filter(query))\n",
    "tf_query = [stemming_query.count(word)for word in vocab]\n",
    "tfidf_query = multiply(tf_query, IDF)\n",
    "print (tf_query)\n",
    "print (tfidf_query)\n",
    "print(files_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6dc025f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test4.TXT', 1), ('test3.TXT', 0.9815025534632754), ('test2.TXT', 0.2701288585084294), ('test1.TXT', 0.20805307689950425), ('test5.TXT', 0.0)]\n",
      "cosine without zeros:\n",
      "[('test4.TXT', 1), ('test3.TXT', 0.9815025534632754), ('test2.TXT', 0.2701288585084294), ('test1.TXT', 0.20805307689950425)]\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "docs = [a_dict[\"tfidf_data\"] for a_dict in files_list]\n",
    "list_cosine_values=[]\n",
    "\n",
    "a_key = \"fileName\"\n",
    "values_of_key = [a_dict[a_key] for a_dict in files_list]\n",
    "\n",
    "for doc in docs:\n",
    "   cosine_value=1 - spatial.distance.cosine(doc, tfidf_query)\n",
    "   list_cosine_values.append(cosine_value)\n",
    "\n",
    "merge_dic=dict(zip(values_of_key, list_cosine_values))\n",
    "sort_merge_dic = sorted(merge_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "result = [i for i in sort_merge_dic if i[1] > 0]#remove the zeros cosine from list of tuples.\n",
    "print(sort_merge_dic)\n",
    "print(\"cosine without zeros:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "105942bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1R 0N:\n",
      "[1, 0, 1, 0, 1]\n",
      "precision:\n",
      "[1.0, 0.5, 0.67, 0.5, 0.6]\n",
      "recall\n",
      "[0.33, 0.33, 0.67, 0.67, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def list_contains(List1, List2): \n",
    "    check = 0\n",
    "    for m in List1:  \n",
    "        for n in List2: \n",
    "            if m == n: \n",
    "                check = 1\n",
    "                return check  \n",
    "                  \n",
    "    return check\n",
    "Relevant_documents=['test4.TXT', 'test2.TXT', 'test5.TXT']\n",
    "\n",
    "R_on=[]\n",
    "for count, i in enumerate (sort_merge_dic): \n",
    "            R_on.append(list_contains(sort_merge_dic[count],Relevant_documents))       \n",
    "print(\"1R 0N:\")\n",
    "print(R_on)\n",
    "\n",
    "#Precision:\n",
    "precision=[]\n",
    "R=0\n",
    "for count, i in enumerate (R_on):\n",
    "    if i==1:\n",
    "        R+=1\n",
    "        precision.append(round(R/(count+1),2))\n",
    "    if i==0:\n",
    "        precision.append(round(R/(count+1),2))\n",
    "print(\"precision:\")\n",
    "print(precision)   \n",
    "\n",
    "#Recall:\n",
    "recall=[]\n",
    "Re=0\n",
    "for count, i in enumerate (R_on):\n",
    "    if i==1:\n",
    "        Re+=1\n",
    "        recall.append(round(Re/len(Relevant_documents),2))\n",
    "    if i==0:\n",
    "        recall.append(round(Re/len(Relevant_documents),2))\n",
    "print(\"recall\")\n",
    "print(recall) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb035d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f94d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c2b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e007ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bu', 'tabl', 'cover', 'switch']\n",
      "Please enter the query:\n",
      "table cover\n",
      "[0.0, 0.5, 0.5, 0.0]\n",
      "[0.0, 0.8047189562170501, 0.8047189562170501, 0.0]\n",
      "[{'fileName': 'test1.TXT', 'data': 'table  bus ', 'stemming_data': ['tabl', 'bu'], 'tf_data': [0.5, 0.5, 0.0, 0.0], 'tfidf_data': [0.8047189562170501, 0.8047189562170501, 0.0, 0.0]}, {'fileName': 'test2.TXT', 'data': 'table  switch', 'stemming_data': ['tabl', 'switch'], 'tf_data': [0.0, 0.5, 0.0, 0.5], 'tfidf_data': [0.0, 0.8047189562170501, 0.0, 0.8047189562170501]}, {'fileName': 'test3.TXT', 'data': 'bus', 'stemming_data': ['bu'], 'tf_data': [1.0, 0.0, 0.0, 0.0], 'tfidf_data': [1.6094379124341003, 0.0, 0.0, 0.0]}, {'fileName': 'test4.TXT', 'data': 'table switch\\n', 'stemming_data': ['tabl', 'switch'], 'tf_data': [0.0, 0.5, 0.0, 0.5], 'tfidf_data': [0.0, 0.8047189562170501, 0.0, 0.8047189562170501]}, {'fileName': 'test5.TXT', 'data': 'table  switch  cover', 'stemming_data': ['tabl', 'switch', 'cover'], 'tf_data': [0.0, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], 'tfidf_data': [0.0, 0.5364793041447, 0.5364793041447, 0.5364793041447]}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from operator import itemgetter\n",
    "with open(\"./stop_words_english.txt\", \"r\",encoding=\"utf8\") as readerst:\n",
    "        datastop_words = readerst.read()    \n",
    "        stop_words = datastop_words.lower().replace(\"\\n\", \" \").split(\" \")\n",
    "files_list = []\n",
    "\n",
    "def filter(data):       \n",
    "    list_of_words = data.lower().replace(\"\\n\", \" \").split(\" \")\n",
    "    filter_list=[word for word in list_of_words if word not in stop_words and len(word) > 1]\n",
    "\n",
    "    for elf in filter_list:\n",
    "        if\".net\"in elf:continue\n",
    "        elif '@'in elf:\n",
    "                if \".com\"in elf or \".lb\" in elf or \".gov\" in elf or \".edu\" in elf:\n",
    "                    continue\n",
    "                else:\n",
    "                    filter_list.remove(elf)    \n",
    "        else:\n",
    "                continue\n",
    "    final_filter_list=[]\n",
    "    for dot in filter_list:\n",
    "            if  dot[-1]==\".\" or dot[-1]==\",\" or dot[-1]==\":\":\n",
    "                dotx = dot[:-1]\n",
    "                dot=dotx\n",
    "                final_filter_list.append(dot)\n",
    "            elif \"'\" in dot:\n",
    "                filter_list.remove(dot)\n",
    "            else:\n",
    "                final_filter_list.append(dot)\n",
    "    filter_list=final_filter_list\n",
    "    \n",
    "    return filter_list\n",
    "\n",
    "def stemming(filter_data):\n",
    "    porter = PorterStemmer()\n",
    "    sufix_list=[(porter.stem(word)) for word in filter_data]\n",
    "    return sufix_list\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for filename in files:\n",
    "        if \"test\" in filename:\n",
    "            file_path = os.path.join(root,filename)\n",
    "            file = open(file_path, 'r')\n",
    "            files_list.append({'fileName': filename, 'data': file.read()})\n",
    "            file.close()\n",
    "def multiply(list1,list2):\n",
    "    import math\n",
    "    products = []\n",
    "    for num1,num2 in zip(list1,list2):\n",
    "        products.append(math.log(num1) * num2)\n",
    "    return products \n",
    "\n",
    "def count_key(key,dict_list):\n",
    "    keys_list = []\n",
    "    for item in dict_list:\n",
    "        keys_list += item.keys()\n",
    "    return keys_list.count(key)   \n",
    "\n",
    "\n",
    "\n",
    "for doc in files_list:\n",
    "    stemming_data = stemming(filter(doc['data']))\n",
    "    doc[\"stemming_data\"] = stemming_data \n",
    "vocab = list(functools.reduce(lambda x ,y : set(x).union(y['stemming_data']), files_list, set()))    \n",
    "print(vocab)\n",
    "for doc in files_list:\n",
    "    stemming_data = doc[\"stemming_data\"]\n",
    "    \n",
    "    tf_data = [stemming_data.count(word)/len(stemming_data) for word in vocab]\n",
    "    doc[\"tf_data\"] = tf_data \n",
    "    \n",
    "    tfidf_data = multiply([count_key(\"fileName\",files_list)/vocab.count(word) for word in vocab],tf_data)\n",
    "    doc[\"tfidf_data\"] = tfidf_data          \n",
    "\n",
    "query = input(\"Please enter the query:\\n\")    \n",
    "stemming_query = stemming(filter(query))\n",
    "tf_query = [stemming_query.count(word)/len(stemming_query) for word in vocab]\n",
    "tfidf_query = multiply([count_key(\"fileName\",files_list)/vocab.count(word) for word in vocab], tf_query)\n",
    "print (tf_query)\n",
    "print (tfidf_query)\n",
    "print(files_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434424cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test5.TXT', 0.816496580927726), ('test1.TXT', 0.5), ('test2.TXT', 0.5), ('test4.TXT', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "docs = [a_dict[\"tfidf_data\"] for a_dict in files_list]\n",
    "list_cosine_values=[]\n",
    "\n",
    "a_key = \"fileName\"\n",
    "values_of_key = [a_dict[a_key] for a_dict in files_list]\n",
    "\n",
    "for doc in docs:\n",
    "   cosine_value=1 - spatial.distance.cosine(doc, tfidf_query)\n",
    "   list_cosine_values.append(cosine_value)\n",
    "\n",
    "merge_dic=dict(zip(values_of_key, list_cosine_values))\n",
    "sort_merge_dic = sorted(merge_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "result = [i for i in sort_merge_dic if i[1] > 0]#remove the zeros cosine from list of tuples.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56795822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
